{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Automated Systematic Review\n",
    "\n",
    "## Introduction\n",
    "### Problem statement and project goals\n",
    "The aim of this project is to develop and evaluate models to reduce the time and therefore the costs of conducting systematic reviews. There are various tools that automate the process of systematic reviews. While these tools mainly apply classical models, our project is aimed to explore deep learning models. \n",
    "\n",
    "Another goal of this project is applying active learning methods and compare them with the classical random approach. Active learning is a type of iterative supervised learning method that select the most informative data points to get labeled by an expert during multiple iterations. In scenarios where not enough labled data is available and manually labeling data is expensive, Active learning is expected to show better performance than random approach. \n",
    "\n",
    "We would like to validate our model on several databases with different sizes and various rates of included papers, rather than relying on a single database.\n",
    "\n",
    "### Approach\n",
    "Our approach is based on the following:\n",
    "- Apply title and abstract screening.\n",
    "- Build models with a pre-trained word embeddings from Wikipedia\n",
    "- Start training models with a defined number of initially included papers.\n",
    "- Run models on High Performance Computers (HPC)\n",
    "\n",
    "\n",
    "### Dataset\n",
    "our model is evaluated on the following datasets:\n",
    "- Systematic Review on Post-Traumatic Stress Disorder (van de Schoot et al., 2018)\n",
    "- 7 systematic drug class reviews (Cohen et al., 2006)\n",
    "- Systematic Review on depression(Cuijpers et al., 2018)\n",
    "\n",
    "### Modeling\n",
    "In this project we developed models using deep learning algorithms and then compared them to classical algorithms based on the following evaluation criteria.    \n",
    "\n",
    "#### Evaluation criteria\n",
    "- minimize the number of relevant papers that is missed, i.e. False Negatives =< 5 \n",
    "- minimize the total number of papers to read including seen and selected papers\n",
    "\n",
    "#### Deep Learning algorithms\n",
    "In this project we explored two types of deep learning algorithms which are commonly applied to various NLP tasks.\n",
    "\n",
    "1. Long Short-Term Memory (LSTM) forward/backward\n",
    "2. Convolutional Neural Network(CNN)\n",
    "\n",
    "These models were built with a pre-trained word embeddings from Wikipedia and tested on various hyperparameters such as various numbers of epochs, batches, dropouts, and different optimization algorithms. We evaluated the models based on the above mentioned criteria. Among the algorithms LSTM backward represented the better results.\n",
    "\n",
    "#### Classical algorithms\n",
    "We compared our Deep Learning model with Rayyan, a web and mobile app for systematic reviews. Rayyan uses Support Vector Machine (SVM) as a classifier.\n",
    "\n",
    "### Paper selection methods\n",
    "There are different ways of selecting papers to present to researchers for labeling. The following are two approaches that we applied.\n",
    "\n",
    "#### Passive Learning\n",
    "Papers are randomly selected from datasets.\n",
    "\n",
    "#### Active Learning\n",
    "Papers are selected for labeling based on a query strategy. A query strategy basically chooses the most informative papers to get the best out of the model when there are not many labeled data available.\n",
    "\n",
    "\n",
    "## Code Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step1 : Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#texts, labels = load_data(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step2: Preprocess texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##tokenize texts\n",
    "# data, word_index = textManager.make_sequences(texts)\n",
    "\n",
    "##create embedding layer\n",
    "##transfer learning\n",
    "# embedding = Word2VecEmbedding(word_index, max_num_words,  max_sequence_length)\n",
    "# embedding.load_word2vec_data(GLOVE_PATH)\n",
    "# embedding_layer = embedding.build_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step3: Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sequence_input = Input(shape=(max_sequence_length, ), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# x = LSTM(\n",
    "#     10,\n",
    "#     input_shape=(max_sequence_length, ),\n",
    "#     go_backwards=backwards,\n",
    "#     dropout=dropout)(embedded_sequences)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# model_lstm = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "# model_lstm.compile(\n",
    "#     loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "# model_lstm.summary()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step4: Split dataset to train/test dataset\n",
    "### Step5: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#passive learning- Papers are selected randomly\n",
    "#x_train, x_val, y_train, y_val = split_data( data, labels, training_size, init_included_papers)\n",
    "\n",
    "# model = LSTM_Model(...)\n",
    "\n",
    "# \"\"\" Train model, calculate scores\"\"\"\n",
    "# model.train(x_train, y_train)\n",
    "# pred = model.prediction(x_val)\n",
    "\n",
    "#or \n",
    "\n",
    "##active learning - Papers are selected based on query strategies\n",
    "# prelabeled_index = select_prelabeled(labels, init_included_papers)\n",
    "# pool = make_pool(data, labels, prelabeled=prelabeled_index)\n",
    "\n",
    "# model = LSTM_Model(...)\n",
    "#init_weights = model._model.get_weights()\n",
    "\n",
    "# while query_i <= args.quota:\n",
    "\n",
    "#         # train the model\n",
    "#         model.train(pool)\n",
    "\n",
    "#         # predict the label of the unlabeled entries in the pool\n",
    "#         idx,features = pool.get_unlabeled_entries()\n",
    "#         pred = model.predict(features)\n",
    "\n",
    "#         # make query\n",
    "#         if (args.query_strategy == 'lc'):\n",
    "#             qs = UncertaintySampling(\n",
    "#                 pool, method='lc', model=model)\n",
    "#         elif (args.query_strategy == 'random'):\n",
    "#             qs = RandomSampling(pool)\n",
    "\n",
    "#         ask_id = qs.make_query(n=args.batch_size)\n",
    "                        \n",
    "#         for i in ask_id:\n",
    "#             lb = int(labels[i][1])\n",
    "#             pool.update(i, lb)\n",
    "\n",
    "         # reset the memory of the model\n",
    "#        model._model.set_weights(init_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step6: Store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#    # save the result to a file\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "#     export_path = os.path.join(\n",
    "#         output_dir, 'dataset_{}_systematic_review_active{}.csv'.format(\n",
    "#             args.dataset, args.T))\n",
    "\n",
    "#     result_df.to_csv(export_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
