{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Automated Systematic Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a pickle file with the data, labels and embedding layer with the\n",
    "following shell command:  \n",
    "\n",
    "``` bash\n",
    "python src/data_prep.py --dataset=[ptsd]\n",
    "```\n",
    "\n",
    "### Passive learning\n",
    "``` \n",
    "python src/systematic_review_passive.py --training_size=[500] --init_included_papers=[10] --dataset=[ptsd]\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "\n",
    "    --training_size: Size of training dataset\n",
    "\n",
    "    --init_included_papers: The number of initially included papers\n",
    "\n",
    "    --dataset: Name of dataset\n",
    "\n",
    "\n",
    "### Active learning \n",
    "\n",
    "- Define prelabeled data points (papers). This step let us to have the same starting point for all query strategies, and have a fair comparison.\n",
    "\n",
    "```\n",
    "select_init_indices.py --dataset=[ptsd] --init_included_papers=[10]\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "\n",
    "    --dataset: Name of dataset\n",
    "\n",
    "    --init_included_papers: The number of initially included papers\n",
    "\n",
    "- Run active learning script\n",
    "```\n",
    "python src/systematic_review_active.py --dataset=[ptsd] --quota=[10] --init_included_papers=[10] \n",
    "--batch_size=[20] --query_strategy=[lc]\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "\n",
    "     --dataset: Name of dataset\n",
    "     \n",
    "     --quota: The number of queries\n",
    "     \n",
    "     --init_included_papers: The number of initially included papers\n",
    "     \n",
    "     --batch_size : Batch size\n",
    "     \n",
    "     --query_strategy: The query strategy name. currently random, lc, lcb and lcbmc are implemented\n",
    "     \n",
    "     See :https://www.sciencedirect.com/science/article/pii/S1532046411001912 \n",
    "\n",
    "\n",
    "## Run simulations on HPC\n",
    "\n",
    "\n",
    "### STEP 0: Setup configuration SurfSara\n",
    "\n",
    "This sections contains an experiment to run a supervised learning simulation\n",
    "on the SurfSara HPC infrastructure.\n",
    "\n",
    "\n",
    "``` bash\n",
    "module load eb\n",
    "module load R\n",
    "module load python/3.5.0-intel\n",
    "```\n",
    "\n",
    "### STEP 1: generate batch files\n",
    "\n",
    "Generate the batch files for active learning, use the following command:\n",
    "\n",
    "``` bash\n",
    "Rscript hpc/make_sr_lstm_batch_active.R [DATASET_NAME] \n",
    "```\n",
    "\n",
    "and for passive learning the following command:\n",
    "\n",
    "``` bash\n",
    "Rscript hpc/make_sr_lstm_batch_passive.R [DATASET_NAME] \n",
    "```\n",
    "\n",
    "Working example: \n",
    "\n",
    "``` bash\n",
    "Rscript hpc/make_sr_lstm_batch_active.R ptsd \n",
    "```\n",
    "\n",
    "\n",
    "### STEP 2: prepare datasets [Locally]\n",
    "\n",
    "To speed up the computations on the HPC, several Python objects are generated\n",
    "beforehand and stored in a pickle file. This file makes it possible to load\n",
    "the objects really fast on each core on the HPC cluster.\n",
    "\n",
    "Create a pickle file with the data, labels and embedding layer with the\n",
    "following shell command:  \n",
    "\n",
    "``` bash\n",
    "python src/data_prep.py --dataset=ptsd\n",
    "```\n",
    "\n",
    "for active learning, Select prelabeled papers\n",
    "\n",
    "```\n",
    "select_init_indices.py --dataset=[ptsd] --init_included_papers=[10]\n",
    "```\n",
    "\n",
    "Run the command locally, such that you do not have to upload the entire word \n",
    "embedding (`wiki.vec`) to the HPC cluster. After running this, upload the 'data_tmp' folder to the cluster.\n",
    "\n",
    "\n",
    "### STEP 3: start simulation\n",
    "\n",
    "Submit the jobs with: \n",
    "\n",
    "passive\n",
    "```bash\n",
    "source batch_files/passive/[dataset_name]/submit_[dataset_name].sh\n",
    "```\n",
    "\n",
    "active learning\n",
    "\n",
    "```bash\n",
    "source batch_files/active_learning/[dataset_name]/submit_[dataset_name].sh\n",
    "```\n",
    "\n",
    "\n",
    "Check the status of the job:\n",
    "\n",
    "```bash \n",
    "squeue -j= [JOB_ID]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
