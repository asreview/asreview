# Automated Systematic Review

*This project is work in progress and **not** production ready.*

Systematic Reviews are “top of the bill” in research. The number of systematic
reviews published by researchers increases year after year. But performing a
sound systematic review is a time-consuming and sometimes boring task. Our
software is designed to take over the step of screening abstracts and titles
with a minimum of papers to be read by a human (in the training set and in the
final included set) and with zero false negatives (or any other small number). 

## Technical documentation

Follows. 

## Planning

- [x] Autumn 2018 - Prototype and test on some labelled data sets. 
- [ ] Winter 2019 – Finalize prototype with active learning + test on some labelled data sets.
- [ ] Spring 2019 – Compare with other software and test with more labelled data sets
- [ ] Autumn/Winter 2019 – Test with unlabeled data and compare to human performance.

## Contact and contributors. 

This project is part of the research work conducted by the Department of Methodology & Statistics,  Faculty of Social and Behavioral Sciences,  Utrecht University,  The Netherlands.

For any questions or remarks, please contact Prof. Dr. Rens van de Schoot (a.g.j.vandeschoot@uu.nl). 

Contributors: 

- Rens van de Schoot (a.g.j.vandeschoot@uu.nl, @Rensvandeschoot)
- Daniel Oberski (d.l.oberski@uu.nl, @doab)
- Parisa Zahedi (p.zahedi@uu.nl, @parisa-zahedi)
- Jonathan de Bruin (@J535D165)
- Kees van Eijden
- Qixiang Fang

## Committing guidelines

  * Commit machine-readable and FOSS formats as much as possible
  * repect dir structure, use the Rstudio project, Makefile etc.
  * Don't commit huge datafiles (>20MB)
  * Communicate on issue tracker and close issues from commits
  * No spaces in filenames please
  * Python scripts in PEP8 and YAPF formatted
 
